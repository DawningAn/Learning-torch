{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "'''\n",
    "这里需要用到数据增强，在transforms模块里实现\n",
    "进一步在transforms里完成数据预处理\n",
    "DataLoader模块直接读取batch数据\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets, models\n",
    "import json\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "'''\n",
    "transforms:常用的图像预处理方法\n",
    "datasets:常用数据集的dataset实现，MNIST，CIFAR-10，Image-net等\n",
    "models:常用的预训练模型，AlexNet，VGG，ResNet，GoogleNet等\n",
    "'''\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 网络模块设置：\n",
    "\n",
    "- 加载预训练模型，torchvision中有很多经典网络架构，调用起来十分方便，并且可以用训练好的权重参数来继续训练，也就是所谓的迁移学习\n",
    "- 需要注意的是别人训练好的任务跟我们自己的任务可不是完全一样，需要把最后的head层改一改，一般也就是最后的全连接层，改成自己的任务\n",
    "- 训练时可以全部重头训练，也可以只训练最后咱们任务的层，因为前几层都是做特征提取的，本质任务目标是一致的\n",
    "\n",
    "### 网络模型保存与测试\n",
    "- 模型保存的时候可以带有选择性，例如在验证集中如果当前效果好则保存\n",
    "- 读取模型进行实际测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# 数据路径定义 ———— 这里的数据集采用同一类数据放在相同的文件夹里，用文件夹的标号来作为label\n",
    "data_dir = './flower_data/'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "data_transforms = {\n",
    "    'train':\n",
    "        transforms.Compose([\n",
    "            transforms.Resize([96, 96]),  # 图片大小裁剪为相同\n",
    "            '''\n",
    "            数据不够，要高效利用现有数据，做数据增强，让数据获得多样性''',\n",
    "            transforms.RandomRotation(45),  # 随机旋转 -45° ~ 45°\n",
    "            transforms.CenterCrop(64),  # 从中心开始裁剪，最后送入模型的大小为64*64\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # 随机水平翻转图像，概率值取0.5\n",
    "            transforms.RandomVerticalFlip(p=0.5),  # 随机垂直翻转，概率值取0.5\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            #参数1为亮度，参数2为对比度，参数3为饱和度，参数4为色相\n",
    "            transforms.RandomGrayscale(p=0.025),  # 概率转换成灰度率，3通道就是R=G=B\n",
    "            transforms.ToTensor(),  # 数据转换为Tensor结构\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 数据标准化(分别对RGB三个通道)，均值，标准差 (x - μ)/σ\n",
    "        ]),\n",
    "    'valid':\n",
    "        transforms.Compose([\n",
    "            transforms.Resize([64, 64]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# 载入数据\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in\n",
    "               ['train', 'valid']}\n",
    "data_size = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "class_name = image_datasets['train'].classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': Dataset ImageFolder\n     Number of datapoints: 6552\n     Root location: ./flower_data/train\n     StandardTransform\n Transform: Compose(\n                Resize(size=[96, 96], interpolation=bilinear, max_size=None, antialias=warn)\n                \n                        数据不够，要高效利用现有数据，做数据增强，让数据获得多样性\n                RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n                CenterCrop(size=(64, 64))\n                RandomHorizontalFlip(p=0.5)\n                RandomVerticalFlip(p=0.5)\n                ColorJitter(brightness=(0.8, 1.2), contrast=(0.9, 1.1), saturation=(0.9, 1.1), hue=(-0.1, 0.1))\n                RandomGrayscale(p=0.025)\n                ToTensor()\n                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ),\n 'valid': Dataset ImageFolder\n     Number of datapoints: 818\n     Root location: ./flower_data/valid\n     StandardTransform\n Transform: Compose(\n                Resize(size=[64, 64], interpolation=bilinear, max_size=None, antialias=warn)\n                ToTensor()\n                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            )}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "['1',\n '10',\n '100',\n '101',\n '102',\n '11',\n '12',\n '13',\n '14',\n '15',\n '16',\n '17',\n '18',\n '19',\n '2',\n '20',\n '21',\n '22',\n '23',\n '24',\n '25',\n '26',\n '27',\n '28',\n '29',\n '3',\n '30',\n '31',\n '32',\n '33',\n '34',\n '35',\n '36',\n '37',\n '38',\n '39',\n '4',\n '40',\n '41',\n '42',\n '43',\n '44',\n '45',\n '46',\n '47',\n '48',\n '49',\n '5',\n '50',\n '51',\n '52',\n '53',\n '54',\n '55',\n '56',\n '57',\n '58',\n '59',\n '6',\n '60',\n '61',\n '62',\n '63',\n '64',\n '65',\n '66',\n '67',\n '68',\n '69',\n '7',\n '70',\n '71',\n '72',\n '73',\n '74',\n '75',\n '76',\n '77',\n '78',\n '79',\n '8',\n '80',\n '81',\n '82',\n '83',\n '84',\n '85',\n '86',\n '87',\n '88',\n '89',\n '9',\n '90',\n '91',\n '92',\n '93',\n '94',\n '95',\n '96',\n '97',\n '98',\n '99']"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# 读取标签对应的名字\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "{'21': 'fire lily',\n '3': 'canterbury bells',\n '45': 'bolero deep blue',\n '1': 'pink primrose',\n '34': 'mexican aster',\n '27': 'prince of wales feathers',\n '7': 'moon orchid',\n '16': 'globe-flower',\n '25': 'grape hyacinth',\n '26': 'corn poppy',\n '79': 'toad lily',\n '39': 'siam tulip',\n '24': 'red ginger',\n '67': 'spring crocus',\n '35': 'alpine sea holly',\n '32': 'garden phlox',\n '10': 'globe thistle',\n '6': 'tiger lily',\n '93': 'ball moss',\n '33': 'love in the mist',\n '9': 'monkshood',\n '102': 'blackberry lily',\n '14': 'spear thistle',\n '19': 'balloon flower',\n '100': 'blanket flower',\n '13': 'king protea',\n '49': 'oxeye daisy',\n '15': 'yellow iris',\n '61': 'cautleya spicata',\n '31': 'carnation',\n '64': 'silverbush',\n '68': 'bearded iris',\n '63': 'black-eyed susan',\n '69': 'windflower',\n '62': 'japanese anemone',\n '20': 'giant white arum lily',\n '38': 'great masterwort',\n '4': 'sweet pea',\n '86': 'tree mallow',\n '101': 'trumpet creeper',\n '42': 'daffodil',\n '22': 'pincushion flower',\n '2': 'hard-leaved pocket orchid',\n '54': 'sunflower',\n '66': 'osteospermum',\n '70': 'tree poppy',\n '85': 'desert-rose',\n '99': 'bromelia',\n '87': 'magnolia',\n '5': 'english marigold',\n '92': 'bee balm',\n '28': 'stemless gentian',\n '97': 'mallow',\n '57': 'gaura',\n '40': 'lenten rose',\n '47': 'marigold',\n '59': 'orange dahlia',\n '48': 'buttercup',\n '55': 'pelargonium',\n '36': 'ruby-lipped cattleya',\n '91': 'hippeastrum',\n '29': 'artichoke',\n '71': 'gazania',\n '90': 'canna lily',\n '18': 'peruvian lily',\n '98': 'mexican petunia',\n '8': 'bird of paradise',\n '30': 'sweet william',\n '17': 'purple coneflower',\n '52': 'wild pansy',\n '84': 'columbine',\n '12': \"colt's foot\",\n '11': 'snapdragon',\n '96': 'camellia',\n '23': 'fritillary',\n '50': 'common dandelion',\n '44': 'poinsettia',\n '53': 'primula',\n '72': 'azalea',\n '65': 'californian poppy',\n '80': 'anthurium',\n '76': 'morning glory',\n '37': 'cape flower',\n '56': 'bishop of llandaff',\n '60': 'pink-yellow dahlia',\n '82': 'clematis',\n '58': 'geranium',\n '75': 'thorn apple',\n '41': 'barbeton daisy',\n '95': 'bougainvillea',\n '43': 'sword lily',\n '83': 'hibiscus',\n '78': 'lotus lotus',\n '88': 'cyclamen',\n '94': 'foxglove',\n '81': 'frangipani',\n '74': 'rose',\n '89': 'watercress',\n '73': 'water lily',\n '46': 'wallflower',\n '77': 'passion flower',\n '51': 'petunia'}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_to_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 加载models中提供的模型，并且直接用训练的好权重当做初始化参数\n",
    "- 第一次执行需要下载，可能会比较慢，我会提供给大家一份下载好的，可以直接放到相应路径"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "model_name = 'resnet'  # 可选的比较多 ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet', 'inception']\n",
    "# 是否用人家训练好的特征来做\n",
    "feature_extract = True  # 都别人模型现成的特征，先不更新\n",
    "# 当其为True时，把模型里其他层都冻住，不进行更新训练，只保留输出层\n",
    "\n",
    "# 是否用GPU训练\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 模型参数要不要更新\n",
    "- 用现成的模型，就一直用了，更不更新可以自己定"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft = models.resnet18()  # 18层的能快点，条件好点的也可以选152\n",
    "model_ft"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# 自己定义哪些地方自己要改，进行更新\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 修改模型输出层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "\n",
    "    num_ftrs = model_ft.fc.in_features  # 找到该模型的FC层，获取这一层的输入512\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 102)  # 类别数自己根据自己任务来\n",
    "\n",
    "    input_size = 64  #输入大小根据自己配置来\n",
    "\n",
    "    return model_ft, input_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\very\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda3\\envs\\very\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\andusk/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# 设置训练模型\n",
    "model_ft, input_size = initialize_model(model_name, 102, feature_extract, use_pretrained=True)\n",
    "\n",
    "#GPU还是CPU计算\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# 模型保存，名字自己起 ———— 主要是保存网络结构和权重参数\n",
    "# C:\\Users\\andusk/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
    "filename = 'best.pth'\n",
    "\n",
    "# 是否训练所有层\n",
    "params_to_update = model_ft.parameters()  # 等于模型中的全部参数\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:  # 当需要去更新时才把这个参数保存，即只保存最后一层fc，进行权重参数更新\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\", name)\n",
    "else:\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\", name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 设置优化器\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m optimizer_ft \u001B[38;5;241m=\u001B[39m \u001B[43moptim\u001B[49m\u001B[38;5;241m.\u001B[39mAdam(params_to_update, lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-2\u001B[39m)  \u001B[38;5;66;03m# 优化参数\u001B[39;00m\n\u001B[0;32m      3\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mStepLR(optimizer_ft, step_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)  \u001B[38;5;66;03m# 学习率每10个epoch衰减成原来的1/10\u001B[39;00m\n\u001B[0;32m      4\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "# 设置优化器\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=1e-2)  # 优化参数\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)  # 学习率每10个epoch衰减成原来的1/10\n",
    "criterion = nn.CrossEntropyLoss()  # 交叉熵损失"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# 训练模块\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, filename='best.pt'):\n",
    "    # 计算一下训练时间\n",
    "    since = time.time()\n",
    "    # 也记录最好的那一次\n",
    "    best_acc = 0\n",
    "    # 模型CPU或者GPU\n",
    "    model.to(device)\n",
    "    # 训练过程中打印一堆损失和指标\n",
    "    val_acc_history = []  # 验证集准确率\n",
    "    train_acc_history = []  # 训练集准确率\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    # 学习率  optimizer.param_groups[0]是个字典结构，lr 为键可以得到当前学习率\n",
    "    LRs = [optimizer.param_groups[0]['lr']]\n",
    "    # 最好的那次模型，后续会变，先初始化\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # 一个一个epoch来遍历\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 训练和验证\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 训练\n",
    "            else:\n",
    "                model.eval()  # 验证\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 把数据都取个遍\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)  # 放到CPU或GPU\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 清零\n",
    "                optimizer.zero_grad()\n",
    "                # 只有训练的时候计算和更新梯度\n",
    "                outputs = model(inputs)  # 102个数 -- 种类\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                # 训练阶段更新权重\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # 计算损失 ---- 当前epoch\n",
    "                running_loss += loss.item() * inputs.size(0)  # 0表示batch那个维度,即batch的大小\n",
    "                running_corrects += torch.sum(preds == labels.data)  #预测结果最大的和真实值是否一致\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)  #算平均\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            time_elapsed = time.time() - since  # 一个epoch浪费了多少时间\n",
    "            print('Time elapsed {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # 得到最好那次的模型\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                state = {\n",
    "                    'state_dict': model.state_dict(),  # 字典里key就是各层的名字，值就是训练好的权重\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }\n",
    "                torch.save(state, filename)\n",
    "            if phase == 'valid':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                valid_losses.append(epoch_loss)\n",
    "                #scheduler.step(epoch_loss)#学习率衰减\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_losses.append(epoch_loss)\n",
    "\n",
    "        print('Optimizer learning rate : {:.7f}'.format(optimizer.param_groups[0]['lr']))\n",
    "        LRs.append(optimizer.param_groups[0]['lr'])\n",
    "        print()\n",
    "        scheduler.step()  #学习率衰减\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # 训练完后用最好的一次当做模型最终的结果,等着一会测试\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, train_acc_history, valid_losses, train_losses, LRs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs = train_model(model_ft, dataloaders,\n",
    "                                                                                            criterion, optimizer_ft,\n",
    "                                                                                            num_epochs=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 再继续训练所有层\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 再继续训练所有的参数，学习率调小一点\n",
    "optimizer = optim.Adam(model_ft.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 加载之前训练好的权重参数\n",
    "\n",
    "checkpoint = torch.load(filename)\n",
    "best_acc = checkpoint['best_acc']\n",
    "model_ft.load_state_dict(checkpoint['state_dict'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs = train_model(model_ft, dataloaders,\n",
    "                                                                                            criterion, optimizer,\n",
    "                                                                                            num_epochs=10, )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
